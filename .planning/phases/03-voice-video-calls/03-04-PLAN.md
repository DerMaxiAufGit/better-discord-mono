---
phase: 03-voice-video-calls
plan: 04
type: execute
wave: 2
depends_on: ["03-02"]
files_modified:
  - frontend/src/lib/webrtc/useAudioDevices.ts
  - frontend/src/lib/webrtc/useAudioLevel.ts
autonomous: true

must_haves:
  truths:
    - "User can enumerate available microphones and speakers"
    - "User can detect microphone activity level"
    - "Device changes are detected automatically"
  artifacts:
    - path: "frontend/src/lib/webrtc/useAudioDevices.ts"
      provides: "Device enumeration and selection"
      exports: ["useAudioDevices"]
      min_lines: 60
    - path: "frontend/src/lib/webrtc/useAudioLevel.ts"
      provides: "Microphone activity detection"
      exports: ["useAudioLevel"]
  key_links:
    - from: "frontend/src/lib/webrtc/useAudioDevices.ts"
      to: "navigator.mediaDevices"
      via: "enumerateDevices"
      pattern: "enumerateDevices"
    - from: "frontend/src/lib/webrtc/useAudioLevel.ts"
      to: "AnalyserNode"
      via: "Web Audio API"
      pattern: "AnalyserNode|getByteFrequencyData"
---

<objective>
Create React hooks for audio device management (enumeration, selection, stream creation) and microphone level detection (for visual activity indicator).

Purpose: Enable users to select audio input/output devices and see visual feedback when their microphone is picking up sound. Required for both settings page (device selection, mic test) and in-call UI (mic activity indicator).

Output: useAudioDevices hook for device management, useAudioLevel hook for mic activity visualization.
</objective>

<execution_context>
@/home/maxi/.claude/get-shit-done/workflows/execute-plan.md
@/home/maxi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-voice-video-calls/03-CONTEXT.md
@.planning/phases/03-voice-video-calls/03-RESEARCH.md
@.planning/phases/03-voice-video-calls/03-02-SUMMARY.md

# Settings store for device preferences
@frontend/src/stores/settingsStore.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create audio devices hook</name>
  <files>frontend/src/lib/webrtc/useAudioDevices.ts</files>
  <action>
Create useAudioDevices.ts hook for device enumeration and stream management:

```typescript
interface AudioDevices {
  inputs: MediaDeviceInfo[];    // Microphones
  outputs: MediaDeviceInfo[];   // Speakers
}

interface UseAudioDevicesReturn {
  devices: AudioDevices;
  isLoading: boolean;
  error: string | null;
  permissionState: 'prompt' | 'granted' | 'denied' | 'unknown';
  refresh: () => Promise<void>;
  getAudioStream: (deviceId?: string) => Promise<MediaStream>;
  setSpeaker: (audioElement: HTMLAudioElement, deviceId: string) => Promise<void>;
}

export function useAudioDevices(): UseAudioDevicesReturn
```

Implementation:
1. On mount, check permission state via navigator.permissions.query({ name: 'microphone' })
2. Enumerate devices with navigator.mediaDevices.enumerateDevices()
3. Filter to audioinput and audiooutput devices
4. Listen for 'devicechange' event to auto-update list
5. Note: Device labels are empty until permission granted - handle gracefully

getAudioStream function:
- Uses constraints from settingsStore (echoCancellation, noiseSuppression, autoGainControl)
- If deviceId provided, use exact constraint
- Returns MediaStream or throws error

setSpeaker function:
- Uses setSinkId on HTMLAudioElement (with TypeScript type assertion)
- Not all browsers support this - check for existence first
  </action>
  <verify>Call useAudioDevices in a component, grant microphone permission, verify inputs array populates with device labels.</verify>
  <done>useAudioDevices provides device enumeration, permission tracking, and stream creation</done>
</task>

<task type="auto">
  <name>Task 2: Create audio level hook for mic activity</name>
  <files>frontend/src/lib/webrtc/useAudioLevel.ts</files>
  <action>
Create useAudioLevel.ts hook for real-time microphone level detection:

```typescript
interface UseAudioLevelReturn {
  level: number;              // 0-100 normalized level
  isActive: boolean;          // true if level > threshold
  start: (stream: MediaStream) => void;
  stop: () => void;
}

export function useAudioLevel(threshold: number = 15): UseAudioLevelReturn
```

Implementation:
1. Create AudioContext and AnalyserNode when start() called
2. Connect MediaStream source to analyser
3. Use requestAnimationFrame loop to poll getByteFrequencyData
4. Calculate average level from frequency data
5. Normalize to 0-100 range
6. Update state at ~30fps (throttle if needed)
7. Cleanup AudioContext on stop() or unmount

The level value powers mic activity indicator (pulsing mic icon).
The isActive boolean provides simple "speaking/not speaking" state.

Handle suspended AudioContext (Chrome requires user gesture):
- Resume context on user interaction if suspended
  </action>
  <verify>Create a test component, call start with a MediaStream, verify level updates when speaking into microphone.</verify>
  <done>useAudioLevel provides real-time microphone level for visual feedback</done>
</task>

</tasks>

<verification>
1. Device list populates after microphone permission granted
2. Device labels visible (not empty) after permission
3. devicechange event triggers re-enumeration
4. getAudioStream returns working MediaStream with audio
5. Audio level updates in real-time when speaking
6. Cleanup properly disposes AudioContext and stops animation frame
</verification>

<success_criteria>
- Both hooks work without errors
- Device enumeration includes proper labels after permission
- Audio level smoothly updates during speech
- No memory leaks from AudioContext or animation frames
</success_criteria>

<output>
After completion, create `.planning/phases/03-voice-video-calls/03-04-SUMMARY.md`
</output>
