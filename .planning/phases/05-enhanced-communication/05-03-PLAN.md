---
phase: 05-enhanced-communication
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - frontend/src/lib/video/videoConstraints.ts
  - frontend/src/lib/video/backgroundBlur.ts
  - frontend/src/hooks/useCamera.ts
autonomous: true

must_haves:
  truths:
    - "Video constraints helper returns getUserMedia options with ideal/max (no exact)"
    - "Background blur uses MediaPipe Selfie Segmentation with canvas output"
    - "useCamera hook manages camera preview with device selection"
  artifacts:
    - path: "frontend/src/lib/video/videoConstraints.ts"
      provides: "Quality presets and constraint builders"
      exports: ["getVideoConstraints", "VideoQuality"]
    - path: "frontend/src/lib/video/backgroundBlur.ts"
      provides: "MediaPipe blur processing"
      exports: ["BackgroundBlurProcessor"]
    - path: "frontend/src/hooks/useCamera.ts"
      provides: "Camera preview management"
      exports: ["useCamera"]
  key_links:
    - from: "useCamera.ts"
      to: "videoConstraints.ts"
      via: "getVideoConstraints import"
      pattern: "import.*getVideoConstraints"
---

<objective>
Create video helper library for camera preview, quality settings, and background blur.

Purpose: Foundation for video call features in Wave 3-5
Output: Reusable video utilities and hooks
</objective>

<execution_context>
@C:\Users\maxi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\maxi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-enhanced-communication/05-RESEARCH.md
@frontend/src/hooks/useAudioDevices.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create video constraints helper</name>
  <files>frontend/src/lib/video/videoConstraints.ts</files>
  <action>
Create frontend/src/lib/video/ directory and videoConstraints.ts:

```typescript
export type VideoQuality = 'low' | 'medium' | 'high'

interface VideoConstraintOptions {
  quality: VideoQuality
  deviceId?: string
  facingMode?: 'user' | 'environment'
}

const QUALITY_PRESETS = {
  low: { width: 640, height: 360, frameRate: 15 },
  medium: { width: 1280, height: 720, frameRate: 24 },
  high: { width: 1920, height: 1080, frameRate: 30 }
} as const

export function getVideoConstraints(options: VideoConstraintOptions): MediaStreamConstraints {
  const preset = QUALITY_PRESETS[options.quality]

  return {
    audio: false,
    video: {
      deviceId: options.deviceId ? { exact: options.deviceId } : undefined,
      facingMode: options.facingMode,
      width: { ideal: preset.width, max: preset.width },
      height: { ideal: preset.height, max: preset.height },
      frameRate: { ideal: preset.frameRate, max: preset.frameRate }
    }
  }
}

export async function getCameraDevices(): Promise<MediaDeviceInfo[]> {
  const devices = await navigator.mediaDevices.enumerateDevices()
  return devices.filter(device => device.kind === 'videoinput')
}

export async function requestCameraPermission(): Promise<boolean> {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ video: true })
    stream.getTracks().forEach(track => track.stop())
    return true
  } catch {
    return false
  }
}
```

Use ideal/max constraints to avoid OverconstrainedError on devices that don't support exact resolution.
  </action>
  <verify>Run: cd frontend && npx tsc --noEmit</verify>
  <done>videoConstraints.ts exports getVideoConstraints, getCameraDevices, requestCameraPermission</done>
</task>

<task type="auto">
  <name>Task 2: Create background blur processor</name>
  <files>frontend/src/lib/video/backgroundBlur.ts</files>
  <action>
Create backgroundBlur.ts with MediaPipe Selfie Segmentation:

```typescript
import { SelfieSegmentation, Results } from '@mediapipe/selfie_segmentation'

export class BackgroundBlurProcessor {
  private segmenter: SelfieSegmentation | null = null
  private outputCanvas: HTMLCanvasElement
  private outputCtx: CanvasRenderingContext2D
  private isProcessing = false
  private animationFrameId: number | null = null
  private blurAmount = 8

  constructor(outputCanvas: HTMLCanvasElement) {
    this.outputCanvas = outputCanvas
    this.outputCtx = outputCanvas.getContext('2d')!
  }

  async initialize(): Promise<void> {
    this.segmenter = new SelfieSegmentation({
      locateFile: (file) =>
        `https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation/${file}`
    })

    this.segmenter.setOptions({
      modelSelection: 1, // landscape model (faster)
      selfieMode: false
    })

    this.segmenter.onResults(this.onResults.bind(this))
  }

  private onResults(results: Results): void {
    const { width, height } = this.outputCanvas

    // Draw blurred background
    this.outputCtx.filter = `blur(${this.blurAmount}px)`
    this.outputCtx.drawImage(results.image, 0, 0, width, height)

    // Draw person without blur using segmentation mask
    this.outputCtx.filter = 'none'
    this.outputCtx.globalCompositeOperation = 'destination-atop'
    this.outputCtx.drawImage(results.segmentationMask, 0, 0, width, height)

    // Reset composite operation
    this.outputCtx.globalCompositeOperation = 'source-over'
  }

  start(videoElement: HTMLVideoElement): void {
    if (this.isProcessing || !this.segmenter) return
    this.isProcessing = true

    // Match canvas to video dimensions
    this.outputCanvas.width = videoElement.videoWidth
    this.outputCanvas.height = videoElement.videoHeight

    const processFrame = async () => {
      if (!this.isProcessing) return

      await this.segmenter!.send({ image: videoElement })

      // Throttle to ~20fps for performance
      this.animationFrameId = requestAnimationFrame(() => {
        setTimeout(processFrame, 50) // 50ms = 20fps
      })
    }

    processFrame()
  }

  stop(): void {
    this.isProcessing = false
    if (this.animationFrameId) {
      cancelAnimationFrame(this.animationFrameId)
      this.animationFrameId = null
    }
  }

  setBlurAmount(amount: number): void {
    this.blurAmount = amount
  }

  getOutputStream(): MediaStream {
    return this.outputCanvas.captureStream(30)
  }

  dispose(): void {
    this.stop()
    this.segmenter?.close()
    this.segmenter = null
  }
}

export function isBlurSupported(): boolean {
  // MediaPipe only works reliably in Chromium browsers
  const isChromium = /Chrome|Chromium|Edg|Opera/.test(navigator.userAgent)
  return isChromium && 'OffscreenCanvas' in window
}
```

Note: 20fps throttle prevents CPU overload while maintaining acceptable smoothness.
  </action>
  <verify>Run: cd frontend && npx tsc --noEmit</verify>
  <done>BackgroundBlurProcessor class exports with initialize, start, stop, getOutputStream</done>
</task>

<task type="auto">
  <name>Task 3: Create useCamera hook</name>
  <files>frontend/src/hooks/useCamera.ts</files>
  <action>
Create useCamera.ts hook for camera preview management:

```typescript
import { useState, useEffect, useCallback, useRef } from 'react'
import { getVideoConstraints, getCameraDevices, VideoQuality } from '@/lib/video/videoConstraints'

interface UseCameraOptions {
  autoStart?: boolean
  quality?: VideoQuality
  deviceId?: string
}

interface UseCameraReturn {
  stream: MediaStream | null
  isActive: boolean
  error: string | null
  devices: MediaDeviceInfo[]
  selectedDeviceId: string | null
  start: () => Promise<void>
  stop: () => void
  switchDevice: (deviceId: string) => Promise<void>
  setQuality: (quality: VideoQuality) => Promise<void>
}

export function useCamera(options: UseCameraOptions = {}): UseCameraReturn {
  const { autoStart = false, quality: initialQuality = 'medium', deviceId: initialDeviceId } = options

  const [stream, setStream] = useState<MediaStream | null>(null)
  const [isActive, setIsActive] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [devices, setDevices] = useState<MediaDeviceInfo[]>([])
  const [selectedDeviceId, setSelectedDeviceId] = useState<string | null>(initialDeviceId ?? null)
  const [quality, setQualityState] = useState<VideoQuality>(initialQuality)

  const streamRef = useRef<MediaStream | null>(null)

  // Load available cameras
  useEffect(() => {
    getCameraDevices().then(setDevices).catch(console.error)
  }, [])

  const stopStream = useCallback(() => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = null
    }
    setStream(null)
    setIsActive(false)
  }, [])

  const startStream = useCallback(async (deviceId?: string, videoQuality?: VideoQuality) => {
    try {
      setError(null)
      stopStream()

      const constraints = getVideoConstraints({
        quality: videoQuality ?? quality,
        deviceId: deviceId ?? selectedDeviceId ?? undefined
      })

      const mediaStream = await navigator.mediaDevices.getUserMedia(constraints)
      streamRef.current = mediaStream
      setStream(mediaStream)
      setIsActive(true)

      if (deviceId) {
        setSelectedDeviceId(deviceId)
      }
    } catch (err) {
      const message = err instanceof Error ? err.message : 'Failed to access camera'
      setError(message)
      console.error('Camera error:', err)
    }
  }, [quality, selectedDeviceId, stopStream])

  const switchDevice = useCallback(async (deviceId: string) => {
    await startStream(deviceId, quality)
  }, [startStream, quality])

  const setQuality = useCallback(async (newQuality: VideoQuality) => {
    setQualityState(newQuality)
    if (isActive) {
      await startStream(selectedDeviceId ?? undefined, newQuality)
    }
  }, [isActive, selectedDeviceId, startStream])

  // Auto-start if requested
  useEffect(() => {
    if (autoStart) {
      startStream()
    }
  }, [autoStart, startStream])

  // Cleanup on unmount
  useEffect(() => {
    return () => stopStream()
  }, [stopStream])

  return {
    stream,
    isActive,
    error,
    devices,
    selectedDeviceId,
    start: () => startStream(),
    stop: stopStream,
    switchDevice,
    setQuality
  }
}
```

Pattern follows existing useAudioDevices hook style.
  </action>
  <verify>Run: cd frontend && npx tsc --noEmit</verify>
  <done>useCamera hook exports with stream management, device switching, quality control</done>
</task>

</tasks>

<verification>
1. frontend/src/lib/video/ directory exists with videoConstraints.ts and backgroundBlur.ts
2. frontend/src/hooks/useCamera.ts exists and exports useCamera
3. TypeScript compiles without errors
4. No runtime imports fail (MediaPipe loaded from CDN)
</verification>

<success_criteria>
- Video constraints use ideal/max, not exact
- BackgroundBlurProcessor uses MediaPipe with 20fps throttle
- useCamera hook manages stream lifecycle with quality presets
- isBlurSupported returns false for non-Chromium browsers
</success_criteria>

<output>
After completion, create `.planning/phases/05-enhanced-communication/05-03-SUMMARY.md`
</output>
